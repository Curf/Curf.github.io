* **Information**:
  *  I(x) = -log(P(x))
* **Entropy**:
  *  Set in x with X descrete states 
  *  H(X) = -[sum _x in X_ P(x)*(log(P(x))]

Coin Flip Ex

Heads: 1/2*log(1/2)

Tails: 1/2*log(1/2)

Entrop = -[Heads + Tails]

* **Cross Entropy**:
  *  Total entropy between 2 distributions
  *  H(P,Q) = -[sum _x in X_ P(x)*(log(Q(x))]

* **KL-Divergence**:
  *  Dkl(P||Q) = -[sum _x in X_ P(x)*(log Q(x)/P(x) )]
  *  Thus, H(P,Q) = H(P) + Dkl(P||Q)
  *  H(P,Q) != H(Q,P)

**Cross-Entropy: Average number of total bits to represent an event from Q instead of P.**

**Relative Entropy (KL Divergence): Average number of extra bits to represent an event from Q instead of P.**
