Having spent the last two years in my M.S. Data Science program, its been a non-stop learning of many new concepts than what I'm used to coming from Chemistry.  I have only recently started to apply these methods on a regular basis (daily) since May, 2020.  I hadn't realized how much of the lower level concepts have leaked out of my head since I was knee-deep in machine learning courses.  And since having my first technical interview I've realized that I should take a step back and hammer it in.  

So, I am creating some posts to help it all make sense (at least to me) and trying to draw only from respected textbooks (**e.g. Goodfellow, Ian, et al. Deep learning**), peer-reviewed articles, and some inspiration from trusted bloggers.

Starting off the basics of statistical approach, and intuitively thinking of a general dataset and of a general algorithm's generalization challenges. 

# ML Basics

## Statistical approaches to modeling

Machine learning modeling can be generally seen as a mathematical solution that best represents some real-world occurrence, function, etc. to complete some task.
And we combine various algorithm components to build an ML model such as...
- An optimization algorithm
- Cost Function
- Model
- and a dataset

We focus on two central approaches to statistics for building these models: 
- Frequentist estimators 
  - Sampling is infinite and decision rules can be sharp. Data are a repeatable random sample - there is a frequency. Underlying parameters are fixed)
- Bayesian inference 
  - Unknown quantities are treated probabilistically and the state of the world can always be updated. Data are observed from the realized sample. Parameters are unknown and described probabilistically. **It is the data which are fixed**)

There are explained in an easy way by [THIS POST](http://www.science.smith.edu/dftwiki/images/d/de/ThemeRiver_StackedGraphs.pdf):
> Here is how I would explain the basic difference to my grandma: 
> 
> I have misplaced my phone somewhere in the home. I can use the phone locator on the base of the instrument to locate the phone and when I press the phone locator the phone starts beeping.
> 
> Problem: Which area of my home should I search?
>
>**Frequentist Reasoning**
>I can hear the phone beeping. I also have a mental model which helps me identify the area from which the sound is coming. Therefore, upon hearing the beep, I infer the area of my home I must search to locate the phone.
>
>**Bayesian Reasoning**
>I can hear the phone beeping. Now, apart from a mental model which helps me identify the area from which the sound is coming from, I also know the locations where I have misplaced the phone in the past. So, I combine my inferences using the beeps and my prior information about the locations I have misplaced the phone in the past to identify an area I must search to locate the phone.

## Overfit, Underfit, capacity background

### Dataset assumptions
We make some assumptions about the training and testing set collection process:
- Both are generated by a probability distribution over datasets called the **data-generating process**
- Assume the _i.i.d. assumptions_:
  - Example in each dataset are **independent** of each other
  - training and testing sets are **identically distributed**, drawn from the same probability distribution
- This creates the shared underlying distribution called **data-generating distribution**,   ![image](https://user-images.githubusercontent.com/26121178/122239295-d211ec80-ce8e-11eb-80c3-18d724f9df74.png).

In theory, this means that our training and testing dataset are sampled from the same distribution, p(x,y), then the expected error should be the same for both datasets for some set parameters.  But for ML we sample the dataset THEN choose parameters to reduce error (not the other way around) --> (test error >= training error)

The ML algorithm must find a balance between training and testing error, which is the challenge of ...
- **Overfitting** - gap between training and testing is too large (model is not general enough and only 'memorizes' the training data)
- **Underfitting** - model cannot obtain a low training error 

Ideally we can control these challenges by altering a model's **Capacity**, or ability to fit a wide variety of functions...
- Low Capacity - underfit, struggling to fit the training set.
- High Capacity - overfit by memorizing properties of training set too much
- **Hypothesis Space** - set of function that learning algorithm is allowed to select as the solution (e.g. linear regression has the set of all linear functions of its input as the hypothesis space)

---

_Including more than just one linear function in Hypothesis space and increasing capacity, left to right (Underfit, just right, Overfit)_

![image](https://user-images.githubusercontent.com/26121178/122256290-e78e1300-ce9c-11eb-994f-27f824c6f509.png)

---

### No Free Lunch Theorem and Regularization

_"No ML algorithm is universally any better than any other"_
For ML, every classification algorithm has the same error rate when classifying unobserved points (averaged over all possible datasets)

**Regularization** - any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.  

In the case of weight decay of L2, we also incorporate a preference for smaller or larger weights (Alpha blending hyperparameter):

---

![image](https://user-images.githubusercontent.com/26121178/122258800-78fe8480-ce9f-11eb-826e-981b7c7ce7b1.png)

---
