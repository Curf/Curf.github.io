# Maximum Likelihood Estimators

Maximum likelihood estimation (MLE), is simply the framework for inference for finding the best statistical estimates of parameters from historical training data.

It is an attempt to make the model distribution match the emperical distibution (p_data) - getting as close to the data generating process as possible.  A benifit of MLE framework is that it has the property of "**consistency**", which means that the model's estimate of parameters improves with the increase in the training set size.

The estimator tries to minimize the dissimilarity between the empirical distribution (p_data) and the model dsitribution with the degree of dissimilarity between the two being measured by [KL-divergence](KLdiv_vs_CrossEntropy.md). We can extend this to cross-entropy, seeing as the parameters altered to minimize cross-entropy is the same as minizing KL-divergence - the difference between the two terms is that KL-divergence includes a term representing the empirical distribution (this is not dependent on the predict distribution).

Two good articles about Likelihood and different loss functions - [applied to deep learning/ANN](https://glassboxmedicine.com/2019/12/07/connections-log-likelihood-cross-entropy-kl-divergence-logistic-regression-and-neural-networks/) and [a more general approach](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/).

---

Suppose we have a random sample _X1, X2...., Xn_ for which the probability density function of each _Xi_ is f(xi|theta).  Then the joint probability density function of _X1, X2...., Xn_, that we call L(theta) is...

![Screen Shot 2021-06-25 at 12 30 39 PM](https://user-images.githubusercontent.com/26121178/123457288-c2428880-d5b1-11eb-9ce5-dad633999f35.png)

So we treat the **Likelihood function**, L(theta) as a function of theta and find the theta tha maximizes it.

---



## Likelihood



<img width="300" alt="Screen Shot 2021-06-18 at 3 23 19 PM" src="https://user-images.githubusercontent.com/26121178/122607822-2b258000-d049-11eb-942b-ad9f95449128.png">


## Logistic Regression 

If we do a binary classification (_positive_ or _negative_) we want our output to give us some probability of the class being _positive_ class. 
This doesn't work with linear regression because we can get large negative or large positive outputs beyond any actual meaning for relative likelihood. Therefore, we need a different function to transform our data. Ideally, this will be a value between 0 and 1 to that we can simply say that the likelihood of _positive_ class is g(x) and the _negative_ class is 1-g(x).

The sigmoid (aka logistic) function works perfectly to achieve this!

---

**Sigmoid Function Equation:**
<img src="https://user-images.githubusercontent.com/26121178/122606731-658e1d80-d047-11eb-896f-bb888b468ed0.png" alt="sig_func" width="300"/> 

<img src="https://user-images.githubusercontent.com/26121178/122606827-91110800-d047-11eb-8d97-9a9bbc79e4c4.png" alt="sig_graph" width="300"/> 

_Large negative inputs tend towards 0 and large positive inputs tend towards 1_

---
