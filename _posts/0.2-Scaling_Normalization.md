# Normalization

Normalization can mean several meanings depending on the context. For ML and Data Science in general, normalization refers to the _shifting_ and _scaling_ versions of statistics, where the intention is to normalize values to allow the comparison of different datasets in a way that eliminates the effects of certain gross influences.  Different types may include only scaling of the data to some relative size variable

* **Standard Score** - Normalizing errors when population parameters are known. Works well for populations that are _normally distributed._  ![Screen Shot 2021-06-24 at 10 17 20 AM](https://user-images.githubusercontent.com/26121178/123279008-76baac80-d4d5-11eb-960e-3a13c44bf2b3.png)
* **Min-Max Scale** - used to bring all values into the range [0,1]. This is also called unity-based normalization. 
 
![image](https://user-images.githubusercontent.com/26121178/123279378-c9946400-d4d5-11eb-81a3-2985a7ec7944.png)

* **Coefficient of variation** - Normalizing dispersion, using the mean as a measure of scale, particularly for positive distribution such as the _exponential distribution_ and _Poisson distribution_.

![image](https://user-images.githubusercontent.com/26121178/123279513-e9c42300-d4d5-11eb-924c-45df24371c44.png)
